{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNOGemS9691Z",
        "outputId": "d3c1a173-9d16-45ae-d1e6-2a33bc9b7f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.25.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.26.14)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n",
            "cp: 'kaggle.json' and '/content/kaggle.json' are the same file\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "! mkdir /content/.kaggle\n",
        "! cp kaggle.json /content/kaggle.json\n",
        "! chmod 600 /content/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c g2net-detecting-continuous-gravitational-waves"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4FsPScoAURZ",
        "outputId": "52d94980-9049-4880-e951-3f63305b1c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/kaggle/api/kaggle_api_extended.py\", line 164, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import h5py\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "import torchvision.transforms as TF\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# Train metadata\n",
        "di = '../input/g2net-detecting-continuous-gravitational-waves'\n",
        "df = pd.read_csv(di + '/train_labels.csv')\n",
        "df = df[df.target >= 0]  # Remove 3 unknowns (target = -1)"
      ],
      "metadata": {
        "id": "xIzPE8aYAdFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms_time_mask = nn.Sequential(\n",
        "                torchaudio.transforms.TimeMasking(time_mask_param=10),\n",
        "            )\n",
        "\n",
        "transforms_freq_mask = nn.Sequential(\n",
        "                torchaudio.transforms.FrequencyMasking(freq_mask_param=10),\n",
        "            )\n",
        "\n",
        "flip_rate = 0.0 # probability of applying the horizontal flip and vertical flip \n",
        "fre_shift_rate = 0.0 # probability of applying the vertical shift\n",
        "\n",
        "time_mask_num = 0 # number of time masking\n",
        "freq_mask_num = 0 # number of frequency masking"
      ],
      "metadata": {
        "id": "m62woZgzKa4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def img_clipping(x, p=3, standard=256):\n",
        "    point = np.percentile(x, 100-p)\n",
        "    imgo = ((standard//2)*(x-x.min())/(point-x.min()))\n",
        "    imgo = np.clip(imgo, 0, standard)\n",
        "    return imgo.astype(np.float32)"
      ],
      "metadata": {
        "id": "eU9wtZ2cKxM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    dataset = Dataset(data_type, df)\n",
        "\n",
        "    img, y = dataset[i]\n",
        "      img (np.float32): 2 x 360 x 128\n",
        "      y (np.float32): label 0 or 1\n",
        "    \"\"\"\n",
        "    def __init__(self, data_type, df, tfms=False):\n",
        "        self.data_type = data_type\n",
        "        self.df = df\n",
        "        self.tfms = tfms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"\n",
        "        i (int): get ith data\n",
        "        \"\"\"\n",
        "        r = self.df.iloc[i]\n",
        "        y = np.float32(r.target)\n",
        "        file_id = r.id\n",
        "\n",
        "        img = np.empty((2, 360, 128), dtype=np.float32)\n",
        "\n",
        "        filename = '%s/%s/%s.hdf5' % (di, self.data_type, file_id)\n",
        "        with h5py.File(filename, 'r') as f:\n",
        "            g = f[file_id]\n",
        "            \n",
        "            for ch, s in enumerate(['H1', 'L1']):\n",
        "                a = g[s]['SFTs'][:, :4096] * 1e22  # Fourier coefficient complex64\n",
        "                p = a.real**2 + a.imag**2  # power\n",
        "                #p /= np.mean(p)  # normalize\n",
        "                p -= np.mean(p)\n",
        "                p = np.mean(p.reshape(360, 128, 32), axis=2)  # compress 4096 -> 128\n",
        "                p = img_clipping(p)\n",
        "                img[ch] = p\n",
        "\n",
        "        if self.tfms:\n",
        "            if np.random.rand() <= flip_rate: # horizontal flip\n",
        "                img = np.flip(img, axis=1).copy()\n",
        "            if np.random.rand() <= flip_rate: # vertical flip\n",
        "                img = np.flip(img, axis=2).copy()\n",
        "            if np.random.rand() <= fre_shift_rate: # vertical shift\n",
        "                img = np.roll(img, np.random.randint(low=0, high=img.shape[1]), axis=1)\n",
        "            \n",
        "            img = torch.from_numpy(img)\n",
        "\n",
        "            for _ in range(time_mask_num): # tima masking\n",
        "                img = transforms_time_mask(img)\n",
        "            for _ in range(freq_mask_num): # frequency masking\n",
        "                img = transforms_freq_mask(img)\n",
        "        \n",
        "        else:\n",
        "            img = torch.from_numpy(img)\n",
        "                \n",
        "        return img, y"
      ],
      "metadata": {
        "id": "jbHYSn1xZyYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "  def __init__(self,img_size,patch_size,in_chans=1,embed_dims=330):\n",
        "    super(PatchEmbed,self).__init__()\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.n_patches = (img_size // patch_size)**2\n",
        "    self.proj = nn.Conv2d(in_chans,embed_dims,kernel_size=patch_size,stride=patch_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.proj(x)\n",
        "    x = x.flatten(2)\n",
        "    x = x.transpose(1,2)\n",
        "    return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self,dim,n_heads=10,qkv_bias=True,attn_p=0.,proj_p=0.):\n",
        "    super(Attention,self).__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.dim = dim\n",
        "    self.head_dim = dim // n_heads\n",
        "    self.scale = self.head_dim**-0.5\n",
        "    self.qkv = nn.Linear(dim,dim*3,bias=qkv_bias)\n",
        "    self.attn_drop = nn.Dropout(attn_p)\n",
        "    self.proj = nn.Linear(dim,dim)\n",
        "    self.proj_drop = nn.Dropout(proj_p)\n",
        "\n",
        "  def forward(self,x):\n",
        "    n_samples,n_tokens,dim = x.shape\n",
        "    if dim != self.dim:\n",
        "      raise ValueError\n",
        "    qkv = self.qkv(x)\n",
        "    qkv = qkv.reshape(n_samples,n_tokens,3,self.n_heads,self.head_dim)\n",
        "    qkv = qkv.permute(2,0,3,1,4)\n",
        "    q,k,v = qkv[0],qkv[1],qkv[2]\n",
        "    k_t = k.transpose(-2,-1)\n",
        "    dp = (q @ k_t)*self.scale\n",
        "    attn = dp.softmax(dim=-1)\n",
        "    weighted_avg = attn @ v\n",
        "    weighted_avg = weighted_avg.transpose(1,2)\n",
        "    weighted_avg = weighted_avg.flatten(2)\n",
        "    x = self.proj(weighted_avg)\n",
        "    x = self.proj_drop(x)\n",
        "    return x\n",
        "  \n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,in_features,hidden_features,out_features,p=0.):\n",
        "    super(MLP,self).__init__()\n",
        "    self.gelu = nn.GELU()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.fc1 = nn.Linear(in_features,hidden_features)\n",
        "    self.fc2 = nn.Linear(hidden_features,hidden_features)\n",
        "    self.fc3 = nn.Linear(hidden_features,hidden_features)\n",
        "    self.fc4 = nn.Linear(hidden_features,out_features)\n",
        "    self.drop = nn.Dropout(p)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.gelu(x)\n",
        "    #x = self.drop(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.gelu(x)\n",
        "    #x = self.drop(x)\n",
        "    x = self.fc3(x)\n",
        "    x = self.gelu(x)\n",
        "    #x = self.drop(x)\n",
        "    x = self.fc4(x)\n",
        "    x = self.sigmoid(x)\n",
        "    #x = self.drop(x)\n",
        "    return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self,dim,n_heads,mlp_ratio=4,qkv_bias=True,p=0.,attn_p=0.):\n",
        "    super(Block,self).__init__()\n",
        "    self.norm1 = nn.LayerNorm(dim,eps=1e-9,)\n",
        "    self.attn = Attention(dim=dim,n_heads=n_heads,qkv_bias=qkv_bias,attn_p=attn_p,proj_p=p)\n",
        "    self.norm2 = nn.LayerNorm(dim,eps=1e-9)\n",
        "    hidden_features = int(dim*mlp_ratio)\n",
        "    self.mlp = MLP(in_features=dim,hidden_features=hidden_features,out_features=dim,p=p)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x + self.attn(self.norm1(x))\n",
        "    x = x + self.mlp(self.norm2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self,img_size=128,patch_size=10,in_chans=1,n_classes=1,embed_dim=64,depth=12,n_heads=10,mlp_ratio=4,qkv_bias=True,p=0.,attn_p=0.):\n",
        "    super(VisionTransformer,self).__init__()\n",
        "    self.patch_embed = PatchEmbed(img_size=img_size,patch_size=patch_size,in_chans=in_chans,embed_dims=330)\n",
        "    self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
        "    self.pos_embed = nn.Parameter(torch.zeros(1,1+self.patch_embed.n_patches,embed_dim))\n",
        "    self.pos_drop = nn.Dropout(p=p)\n",
        "    self.blocks = nn.ModuleList([Block(dim=embed_dim,n_heads=n_heads,qkv_bias=qkv_bias,p=p,attn_p=p)for j in range(depth)])\n",
        "    self.norm = nn.LayerNorm(embed_dim,eps=1e-6)\n",
        "    self.head = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    n_samples = x.shape[0]\n",
        "    x = self.patch_embed(x)\n",
        "    cls_token = self.cls_token.expand(n_samples,-1,-1)\n",
        "    x = torch.cat((cls_token,x),dim=1)\n",
        "    x = x + self.pos_embed\n",
        "    x = self.pos_drop(x)\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "    x = self.norm(x)\n",
        "    cls_token_final = x[:,0]\n",
        "    x = self.head(cls_token_final)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "wTaiYhixbITh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 25\n",
        "batch_size = 16\n",
        "num_workers = 2\n",
        "dataset_train = Dataset('train', df, tfms=True)\n",
        "training_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "lv-KehEJmtDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#custom_config = {\n",
        "#       \"img_size\": 165,\n",
        "#        \"in_chans\": 1,\n",
        "#        \"patch_size\": 10,\n",
        "#        \"embed_dim\": 330,\n",
        "#        \"depth\": 12,\n",
        "#        \"n_heads\": 10,\n",
        "#        \"qkv_bias\": True,\n",
        "#        \"mlp_ratio\": 4,\n",
        "#}\n",
        "\n",
        "model = VisionTransformer()\n",
        "learning_rate = 0.002\n",
        "optimiser = torch.optim.AdamW(model.parameters(),lr = learning_rate,weight_decay=1e-6)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i,(X,y) in enumerate(training_loader):\n",
        "    y_predicted = model(X)\n",
        "    loss = criterion(y_predicted,y)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    print('iteration: ',i)\n",
        "  if epoch == epochs:\n",
        "    print('Training Complete')\n",
        "\n",
        "torch.save(model,'/content/model')"
      ],
      "metadata": {
        "id": "PdWC65sZfq1Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}